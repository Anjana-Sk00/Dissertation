# -*- coding: utf-8 -*-
"""Model1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gMIxfCZveAn753cR70W6IipjyZCA4cS3
"""

!pip install --upgrade pip
!pip install numpy pandas opencv-python matplotlib scikit-learn

!pip install tensorflow pillow seaborn

# Step 2: Import necessary libraries for data handling, image processing, and visualization
import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layers, models, Input
from tensorflow.keras.applications import VGG16
from tensorflow.keras.optimizers import Adam
from google.colab import drive
import zipfile

drive.mount('/content/drive')

# Step 2: Define paths for the zip files and where to temporarily unzip them in Colab
import os

drive_zip_path = '/content/drive/MyDrive/'  # Folder in Google Drive where the zip files are stored
colab_unzip_path = '/content/unzipped_data/'  # Temporary storage in Colab
processed_faces_path = '/content/processed_faces/'  # Path to save processed face images

# Create necessary directories if they do not exist
if not os.path.exists(processed_faces_path):
    os.makedirs(processed_faces_path)

import wget

# Base URL for the files on Zenodo (change this if the URL structure is different)
base_url = 'https://zenodo.org/record/1188976/files/Video_Speech_Actor_'

# Download each zip file for actors 01 to 24
for i in range(1, 25):  # Actors 01 to 24
    actor_num = str(i).zfill(2)  # Format actor number as 2 digits (e.g., '01', '02', ..., '24')
    url = base_url + actor_num + '.zip?download=1'

    # Path to save the file in Google Drive
    output_path = f'/content/drive/MyDrive/ravdess_video_actor_{actor_num}.zip'

    print(f"Downloading {url} to {output_path}")
    wget.download(url, output_path)

# Step 3: Define the list of zip files to process
import zipfile

# List of zip files (actors 01 to 24)
zip_files = [f"ravdess_video_actor_{str(i).zfill(2)}.zip" for i in range(1, 25)]

# Function to process each zip file by unzipping it in Colab
def process_zip_file(zip_file):
    zip_file_path = os.path.join(drive_zip_path, zip_file)

    # Unzip the current zip file into Colab's temporary storage
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        print(f"Unzipping {zip_file}...")
        zip_ref.extractall(colab_unzip_path)

# Process a few zip files at a time to avoid running out of space
for zip_file in zip_files:  # Adjust the range to control the number of files processed at once
    process_zip_file(zip_file)

import os

# Directory where unzipped folders are located
colab_unzip_path = '/content/unzipped_data/'  # Path where unzipped RAVDESS data is stored

# Dictionary to store the number of videos per unzipped folder
video_counts_per_folder = {}

# Function to count videos in each unzipped folder
def count_videos_in_folders():
    for root, dirs, files in os.walk(colab_unzip_path):
        for folder in dirs:
            folder_path = os.path.join(root, folder)
            video_count = 0

            # Loop through the files in the current folder
            for file in os.listdir(folder_path):
                if file.endswith('.mp4'):  # Count only the video files (e.g., .mp4)
                    video_count += 1

            # Store the count for this folder
            video_counts_per_folder[folder] = video_count

# Run the function to count videos
count_videos_in_folders()

# Print the number of videos per folder
print("\nNumber of videos in each unzipped folder:")
for folder, video_count in video_counts_per_folder.items():
    print(f"{folder}: {video_count} videos")

# Optionally, you can also print the total number of videos across all folders
total_videos = sum(video_counts_per_folder.values())
print(f"\nTotal number of videos across all folders: {total_videos}")

#code for frame extraction
import os
import cv2

# Define emotion labels based on the known dataset structure
emotion_dict = {
    '01': 'neutral',
    '02': 'calm',
    '03': 'happy',
    '04': 'sad',
    '05': 'angry',
    '06': 'fearful',
    '07': 'disgust',
    '08': 'surprised'
}

# Directory paths
colab_unzip_path = '/content/unzipped_data/'  # Path where the unzipped RAVDESS data is stored
processed_faces_path = '/content/processed_faces/'  # Path to save the extracted frames

# Create directory if it doesn't exist
if not os.path.exists(processed_faces_path):
    os.makedirs(processed_faces_path)

# Counter for skipped videos
videos_skipped = 0
videos_processed = 0

# Function to extract all frames from each video with correct labeling
def extract_frames_with_labels(video_file, actor_id, video_id, emotion_label):
    cap = cv2.VideoCapture(video_file)
    frame_count = 0

    # Loop through all frames in the video
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break  # End of the video

        # Save every frame with a unique identifier
        frame_filename = f"{emotion_label}_Actor_{actor_id}_video_{video_id}_frame_{frame_count}.jpg"
        frame_path = os.path.join(processed_faces_path, frame_filename)
        if cv2.imwrite(frame_path, frame):  # Save the frame as an image
            print(f"Saved {frame_filename}")
        else:
            print(f"Failed to save {frame_filename}")
        frame_count += 1

    cap.release()
    print(f"Extracted {frame_count} frames from {video_file} (Expected: {frame_count})")

# Loop through all the video files and extract frames
for root, dirs, files in os.walk(colab_unzip_path):
    for file in files:
        if file.endswith('.mp4'):
            video_file_path = os.path.join(root, file)

            try:
                # Extract actor ID and video ID from the filename
                actor_id = file.split('-')[6]  # Actor number is the 7th part in the filename
                video_id = file.split('-')[1]  # Video ID (which variant of the expression)

                # Extract the emotion code from the video filename (e.g., '03' in '03-01-02-01-01-01-01.mp4')
                emotion_code = file.split('-')[2]
                emotion_label = emotion_dict.get(emotion_code, 'unknown')  # Look up the emotion label

                # Check if the emotion label is 'unknown' (which means the emotion code was invalid)
                if emotion_label == 'unknown':
                    raise ValueError(f"Invalid emotion code in {file}")

                # Print the current file being processed
                print(f"Processing {file} with label {emotion_label}...")

                # Extract frames from the video
                extract_frames_with_labels(video_file_path, actor_id, video_id, emotion_label)
                videos_processed += 1

            except (IndexError, ValueError) as e:
                # Increment the skipped video counter and print a message
                videos_skipped += 1
                print(f"Failed to process {file}. Skipping... Error: {str(e)}")

# Print the total number of skipped videos and processed videos
print(f"\nTotal number of videos processed: {videos_processed}")
print(f"Total number of videos skipped: {videos_skipped}")

# Counting the total number of frames extracted
def count_extracted_frames():
    frame_counts_per_video = {}
    total_frames_extracted = 0

    # Loop through each file in the processed_faces_path directory
    for img_file in os.listdir(processed_faces_path):
        # Extract the unique video identifier from the filename (actor + video identifier)
        # Example filename: "happy_Actor_08_video_03_frame_0.jpg"
        video_name = "_".join(img_file.split('_frame_')[0].split('_')[:4])  # Includes actor, emotion, and video identifier

        # Increment the frame count for this video
        if video_name in frame_counts_per_video:
            frame_counts_per_video[video_name] += 1
        else:
            frame_counts_per_video[video_name] = 1

        # Increment the total frames count
        total_frames_extracted += 1

    # Print the total number of frames extracted per video
    print("\nTotal frames extracted per video:")
    for video, frame_count in frame_counts_per_video.items():
        print(f"{video}: {frame_count} frames")

    # Print the overall total number of frames extracted
    print(f"\nOverall total frames extracted: {total_frames_extracted}")

# Run the frame counting function
count_extracted_frames()

import os

# Directory where extracted frames are stored
processed_faces_path = '/content/processed_faces/'

# Check if the directory exists
if os.path.exists(processed_faces_path):
    # List the files in the directory
    files = os.listdir(processed_faces_path)
    print(f"Total number of files (frames) in {processed_faces_path}: {len(files)}")
else:
    print(f"The directory {processed_faces_path} does not exist.")

pip install dlib

!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2

!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2

!ls

import dlib
predictor = dlib.shape_predictor("/content/shape_predictor_68_face_landmarks.dat")

#code for landmark extraction
import dlib
import cv2
import os

# Load pre-trained face detector and landmark predictor from dlib
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("/content/shape_predictor_68_face_landmarks.dat")  # Update this path if needed

# Directory where extracted frames are stored
processed_faces_path = '/content/processed_faces/'

# Create directory for saving landmarks visualization if needed
landmark_output_path = '/content/landmarks_faces/'
if not os.path.exists(landmark_output_path):
    os.makedirs(landmark_output_path)

def extract_facial_landmarks(image, image_filename):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = detector(gray)

    for face in faces:
        landmarks = predictor(gray, face)
        landmark_points = []
        for n in range(0, 68):  # 68 facial landmarks
            x = landmarks.part(n).x
            y = landmarks.part(n).y
            landmark_points.append((x, y))

            # Draw the landmark points on the image (optional for visualization)
            cv2.circle(image, (x, y), 2, (255, 0, 0), -1)

        # Save the image with landmarks drawn (optional)
        landmark_img_path = os.path.join(landmark_output_path, image_filename)
        cv2.imwrite(landmark_img_path, image)

        return landmark_points
    return None

# Loop through the frames directory and apply landmark detection
for img_file in os.listdir(processed_faces_path):
    if img_file.endswith('.jpg'):
        image_path = os.path.join(processed_faces_path, img_file)
        image = cv2.imread(image_path)

        landmarks = extract_facial_landmarks(image, img_file)
        if landmarks:
            print(f"Extracted landmarks from {img_file}")
        else:
            print(f"No face detected in {img_file}")

#code for data preparation
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
import cv2

# Directory where extracted frames are stored
processed_faces_path = '/content/processed_faces/'

# Prepare data and labels
data = []
labels = []

# Loop through the frames directory and prepare data
for img_file in os.listdir(processed_faces_path):
    if img_file.endswith('.jpg'):
        image_path = os.path.join(processed_faces_path, img_file)
        image = cv2.imread(image_path)

        if image is not None:
            # Resize image to a consistent size (e.g., 128x128)
            image_resized = cv2.resize(image, (128, 128))
            data.append(image_resized)

            # Extract label from filename (e.g., "happy_Actor_08_video_03_frame_0.jpg")
            label = img_file.split('_')[0]
            labels.append(label)
        else:
            print(f"Could not read {img_file}")

# Convert data to numpy arrays
data = np.array(data, dtype="float32") / 255.0  # Normalize pixel values to [0, 1]
labels = np.array(labels)

# Encode labels
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)
labels_encoded = to_categorical(labels_encoded)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, labels_encoded, test_size=0.2, random_state=42)

# Print out shapes to verify the split
print(f"Training data shape: {X_train.shape}")
print(f"Test data shape: {X_test.shape}")
print(f"Training labels shape: {y_train.shape}")
print(f"Test labels shape: {y_test.shape}")

#code for training model
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Define a CNN model from scratch
def create_cnn_model(input_shape, num_classes):
    model = models.Sequential()

    # First Convolutional Block
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Conv2D(32, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Second Convolutional Block
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Third Convolutional Block
    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Flatten and Fully Connected Layers
    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dropout(0.5))  # Dropout for regularization
    model.add(layers.Dense(num_classes, activation='softmax'))

    return model

# Set input shape and number of classes based on your dataset
input_shape = (128, 128, 3)  # Images are resized to 128x128 with 3 color channels
num_classes = y_train.shape[1]  # Number of emotion categories

# Create the model
model = create_cnn_model(input_shape, num_classes)

# Compile the model with a custom learning rate
learning_rate = 0.0001
optimizer = Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Data augmentation to reduce overfitting
data_generator = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Train the model with data augmentation
batch_size = 16
epochs = 100
history = model.fit(data_generator.flow(X_train, y_train, batch_size=batch_size),
                    validation_data=(X_test, y_test),
                    epochs=epochs,
                    batch_size=batch_size)

# Save the trained model
model.save('emotion_recognition_model.h5')

# Print the model summary
model.summary()

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.show()

#code for testing
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import tensorflow as tf

# Load the trained model
model = tf.keras.models.load_model('emotion_recognition_model.h5')

# Recompile the model to ensure metrics are built
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Evaluate the model to build the metrics (required to avoid the warning)
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Set up labels
emotion_categories = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']

# Make predictions on the test set
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)

# Calculate accuracy
accuracy = np.sum(predicted_classes == true_classes) / len(true_classes)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

# Generate classification report
print("\nClassification Report:")
print(classification_report(true_classes, predicted_classes, target_names=emotion_categories))

# Generate confusion matrix
conf_matrix = confusion_matrix(true_classes, predicted_classes)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=emotion_categories, yticklabels=emotion_categories)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

model_path = '/content/drive/MyDrive/emotion_recognition_model.h5'
model.save(model_path)
print(f'Model saved to: {model_path}')